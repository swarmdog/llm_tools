[Skip to content](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#human-in-the-loop)

[Edit this page](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/concepts/human_in_the_loop.md "Edit this page")

# Human-in-the-loop [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#human-in-the-loop "Permanent link")

This guide uses the new `interrupt` function.

As of LangGraph 0.2.57, the recommended way to set breakpoints is using the [`interrupt` function](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt) as it simplifies **human-in-the-loop** patterns.

If you're looking for the previous version of this conceptual guide, which relied on static breakpoints and `NodeInterrupt` exception, it is available [here](https://langchain-ai.github.io/langgraph/concepts/v0-human-in-the-loop/).

A **human-in-the-loop** (or "on-the-loop") workflow integrates human input into automated processes, allowing for decisions, validation, or corrections at key stages. This is especially useful in **LLM-based applications**, where the underlying model may generate occasional inaccuracies. In low-error-tolerance scenarios like compliance, decision-making, or content generation, human involvement ensures reliability by enabling review, correction, or override of model outputs.

## Use cases [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#use-cases "Permanent link")

Key use cases for **human-in-the-loop** workflows in LLM-based applications include:

1. [**ðŸ› ï¸ Reviewing tool calls**](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#review-tool-calls): Humans can review, edit, or approve tool calls requested by the LLM before tool execution.
2. **âœ… Validating LLM outputs**: Humans can review, edit, or approve content generated by the LLM.
3. **ðŸ’¡ Providing context**: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.

## `interrupt` [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#interrupt "Permanent link")

The [`interrupt` function](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt) in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, presenting information to a human, and resuming the graph with their input. This function is useful for tasks like approvals, edits, or collecting additional input. The [`interrupt` function](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt) is used in conjunction with the [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object to resume the graph with a value provided by the human.

```md-code__content
from langgraph.types import interrupt

def human_node(state: State):
    value = interrupt(
        # Any JSON serializable value to surface to the human.
        # For example, a question or a piece of text or a set of keys in the state
       {
          "text_to_revise": state["some_text"]
       }
    )
    # Update the state with the human's input or route the graph based on the input.
    return {
        "some_text": value
    }

graph = graph_builder.compile(
    checkpointer=checkpointer # Required for `interrupt` to work
)

# Run the graph until the interrupt
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(some_input, config=thread_config)

# Resume the graph with the human's input
graph.invoke(Command(resume=value_from_human), config=thread_config)

```

API Reference: [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt)

```md-code__content
{'some_text': 'Edited text'}

```

Warning

Interrupts are both powerful and ergonomic. However, while they may resemble Python's input() function in terms of developer experience, it's important to note that they do not automatically resume execution from the interruption point. Instead, they rerun the entire node where the interrupt was used.
For this reason, interrupts are typically best placed at the start of a node or in a dedicated node. Please read the [resuming from an interrupt](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#how-does-resuming-from-an-interrupt-work) section for more details.

Full Code

Here's a full example of how to use `interrupt` in a graph, if you'd like
to see the code in action.

```md-code__content
from typing import TypedDict
import uuid

from langgraph.checkpoint.memory import MemorySaver
from langgraph.constants import START
from langgraph.graph import StateGraph
from langgraph.types import interrupt, Command

class State(TypedDict):
   """The graph state."""
   some_text: str

def human_node(state: State):
   value = interrupt(
      # Any JSON serializable value to surface to the human.
      # For example, a question or a piece of text or a set of keys in the state
      {
         "text_to_revise": state["some_text"]
      }
   )
   return {
      # Update the state with the human's input
      "some_text": value
   }

# Build the graph
graph_builder = StateGraph(State)
# Add the human-node to the graph
graph_builder.add_node("human_node", human_node)
graph_builder.add_edge(START, "human_node")

# A checkpointer is required for `interrupt` to work.
checkpointer = MemorySaver()
graph = graph_builder.compile(
   checkpointer=checkpointer
)

# Pass a thread ID to the graph to run it.
thread_config = {"configurable": {"thread_id": uuid.uuid4()}}

# Using stream() to directly surface the `__interrupt__` information.
for chunk in graph.stream({"some_text": "Original text"}, config=thread_config):
   print(chunk)

# Resume using Command
for chunk in graph.stream(Command(resume="Edited text"), config=thread_config):
   print(chunk)

```

```md-code__content
{'__interrupt__': (
      Interrupt(
         value={'question': 'Please revise the text', 'some_text': 'Original text'},
         resumable=True,
         ns=['human_node:10fe492f-3688-c8c6-0d0a-ec61a43fecd6'],
         when='during'
      ),
   )
}
{'human_node': {'some_text': 'Edited text'}}

```

## Requirements [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#requirements "Permanent link")

To use `interrupt` in your graph, you need to:

1. [**Specify a checkpointer**](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints) to save the graph state after each step.
2. **Call `interrupt()`** in the appropriate place. See the [Design Patterns](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#design-patterns) section for examples.
3. **Run the graph** with a [**thread ID**](https://langchain-ai.github.io/langgraph/concepts/persistence/#threads) until the `interrupt` is hit.
4. **Resume execution** using `invoke`/ `ainvoke`/ `stream`/ `astream` (see [**The `Command` primitive**](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#the-command-primitive)).

## Design Patterns [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#design-patterns "Permanent link")

There are typically three different **actions** that you can do with a human-in-the-loop workflow:

1. **Approve or Reject**: Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involve **routing** the graph based on the human's input.
2. **Edit Graph State**: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves **updating** the state with the human's input.
3. **Get Input**: Explicitly request human input at a particular step in the graph. This is useful for collecting additional information or context to inform the agent's decision-making process or for supporting **multi-turn conversations**.

Below we show different design patterns that can be implemented using these **actions**.

### Approve or Reject [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#approve-or-reject "Permanent link")

![image](https://langchain-ai.github.io/langgraph/concepts/img/human_in_the_loop/approve-or-reject.png)

Depending on the human's approval or rejection, the graph can proceed with the action or take an alternative path.

Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action.

```md-code__content
from typing import Literal
from langgraph.types import interrupt, Command

def human_approval(state: State) -> Command[Literal["some_node", "another_node"]]:
    is_approved = interrupt(
        {
            "question": "Is this correct?",
            # Surface the output that should be
            # reviewed and approved by the human.
            "llm_output": state["llm_output"]
        }
    )

    if is_approved:
        return Command(goto="some_node")
    else:
        return Command(goto="another_node")

# Add the node to the graph in an appropriate location
# and connect it to the relevant nodes.
graph_builder.add_node("human_approval", human_approval)
graph = graph_builder.compile(checkpointer=checkpointer)

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with either an approval or rejection.
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(Command(resume=True), config=thread_config)

```

API Reference: [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt) \| [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command)

See [how to review tool calls](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/) for a more detailed example.

### Review & Edit State [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#review-edit-state "Permanent link")

![image](https://langchain-ai.github.io/langgraph/concepts/img/human_in_the_loop/edit-graph-state-simple.png)

A human can review and edit the state of the graph. This is useful for correcting mistakes or updating the state with additional information.

```md-code__content
from langgraph.types import interrupt

def human_editing(state: State):
    ...
    result = interrupt(
        # Interrupt information to surface to the client.
        # Can be any JSON serializable value.
        {
            "task": "Review the output from the LLM and make any necessary edits.",
            "llm_generated_summary": state["llm_generated_summary"]
        }
    )

    # Update the state with the edited text
    return {
        "llm_generated_summary": result["edited_text"]
    }

# Add the node to the graph in an appropriate location
# and connect it to the relevant nodes.
graph_builder.add_node("human_editing", human_editing)
graph = graph_builder.compile(checkpointer=checkpointer)

...

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with the edited text.
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke(
    Command(resume={"edited_text": "The edited text"}),
    config=thread_config
)

```

API Reference: [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt)

See [How to wait for user input using interrupt](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/) for a more detailed example.

### Review Tool Calls [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#review-tool-calls "Permanent link")

![image](https://langchain-ai.github.io/langgraph/concepts/img/human_in_the_loop/tool-call-review.png)

A human can review and edit the output from the LLM before proceeding. This is particularly
critical in applications where the tool calls requested by the LLM may be sensitive or require human oversight.

```md-code__content
def human_review_node(state) -> Command[Literal["call_llm", "run_tool"]]:
    # This is the value we'll be providing via Command(resume=<human_review>)
    human_review = interrupt(
        {
            "question": "Is this correct?",
            # Surface tool calls for review
            "tool_call": tool_call
        }
    )

    review_action, review_data = human_review

    # Approve the tool call and continue
    if review_action == "continue":
        return Command(goto="run_tool")

    # Modify the tool call manually and then continue
    elif review_action == "update":
        ...
        updated_msg = get_updated_msg(review_data)
        # Remember that to modify an existing message you will need
        # to pass the message with a matching ID.
        return Command(goto="run_tool", update={"messages": [updated_message]})

    # Give natural language feedback, and then pass that back to the agent
    elif review_action == "feedback":
        ...
        feedback_msg = get_feedback_msg(review_data)
        return Command(goto="call_llm", update={"messages": [feedback_msg]})

```

See [how to review tool calls](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/) for a more detailed example.

### Multi-turn conversation [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#multi-turn-conversation "Permanent link")

![image](https://langchain-ai.github.io/langgraph/concepts/img/human_in_the_loop/multi-turn-conversation.png)

A **multi-turn conversation** architecture where an **agent** and **human node** cycle back and forth until the agent decides to hand off the conversation to another agent or another part of the system.

A **multi-turn conversation** involves multiple back-and-forth interactions between an agent and a human, which can allow the agent to gather additional information from the human in a conversational manner.

This design pattern is useful in an LLM application consisting of [multiple agents](https://langchain-ai.github.io/langgraph/concepts/multi_agent/). One or more agents may need to carry out multi-turn conversations with a human, where the human provides input or feedback at different stages of the conversation. For simplicity, the agent implementation below is illustrated as a single node, but in reality
it may be part of a larger graph consisting of multiple nodes and include a conditional edge.

[Using a human node per agent](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#__tabbed_1_1)[Sharing human node across multiple agents](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#__tabbed_1_2)

In this pattern, each agent has its own human node for collecting user input.
This can be achieved by either naming the human nodes with unique names (e.g., "human for agent 1", "human for agent 2") or by
using subgraphs where a subgraph contains a human node and an agent node.

```md-code__content
from langgraph.types import interrupt

def human_input(state: State):
    human_message = interrupt("human_input")
    return {
        "messages": [\
            {\
                "role": "human",\
                "content": human_message\
            }\
        ]
    }

def agent(state: State):
    # Agent logic
    ...

graph_builder.add_node("human_input", human_input)
graph_builder.add_edge("human_input", "agent")
graph = graph_builder.compile(checkpointer=checkpointer)

# After running the graph and hitting the interrupt, the graph will pause.
# Resume it with the human's input.
graph.invoke(
    Command(resume="hello!"),
    config=thread_config
)

```

In this pattern, a single human node is used to collect user input for multiple agents. The active agent is determined from the state, so after human input is collected, the graph can route to the correct agent.

```md-code__content
from langgraph.types import interrupt

def human_node(state: MessagesState) -> Command[Literal["agent_1", "agent_2", ...]]:
    """A node for collecting user input."""
    user_input = interrupt(value="Ready for user input.")

    # Determine the **active agent** from the state, so
    # we can route to the correct agent after collecting input.
    # For example, add a field to the state or use the last active agent.
    # or fill in `name` attribute of AI messages generated by the agents.
    active_agent = ...

    return Command(
        update={
            "messages": [{\
                "role": "human",\
                "content": user_input,\
            }]
        },
        goto=active_agent,
    )

```

See [how to implement multi-turn conversations](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/) for a more detailed example.

### Validating human input [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#validating-human-input "Permanent link")

If you need to validate the input provided by the human within the graph itself (rather than on the client side), you can achieve this by using multiple interrupt calls within a single node.

```md-code__content
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""
    question = "What is your age?"

    while True:
        answer = interrupt(question)

        # Validate answer, if the answer isn't valid ask for input again.
        if not isinstance(answer, int) or answer < 0:
            question = f"'{answer} is not a valid age. What is your age?"
            answer = None
            continue
        else:
            # If the answer is valid, we can proceed.
            break

    print(f"The human in the loop is {answer} years old.")
    return {
        "age": answer
    }

```

API Reference: [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt)

## The `Command` primitive [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#the-command-primitive "Permanent link")

When using the `interrupt` function, the graph will pause at the interrupt and wait for user input.

Graph execution can be resumed using the [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) primitive which can be passed through the `invoke`, `ainvoke`, `stream` or `astream` methods.

The `Command` primitive provides several options to control and modify the graph's state during resumption:

1. **Pass a value to the `interrupt`**: Provide data, such as a user's response, to the graph using `Command(resume=value)`. Execution resumes from the beginning of the node where the `interrupt` was used, however, this time the `interrupt(...)` call will return the value passed in the `Command(resume=value)` instead of pausing the graph.



```md-code__content
# Resume graph execution with the user's input.
graph.invoke(Command(resume={"age": "25"}), thread_config)

```

2. **Update the graph state**: Modify the graph state using `Command(update=update)`. Note that resumption starts from the beginning of the node where the `interrupt` was used. Execution resumes from the beginning of the node where the `interrupt` was used, but with the updated state.



```md-code__content
# Update the graph state and resume.
# You must provide a `resume` value if using an `interrupt`.
graph.invoke(Command(update={"foo": "bar"}, resume="Let's go!!!"), thread_config)

```


By leveraging `Command`, you can resume graph execution, handle user inputs, and dynamically adjust the graph's state.

## Using with `invoke` and `ainvoke` [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#using-with-invoke-and-ainvoke "Permanent link")

When you use `stream` or `astream` to run the graph, you will receive an `Interrupt` event that let you know the `interrupt` was triggered.

`invoke` and `ainvoke` do not return the interrupt information. To access this information, you must use the [get\_state](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph.get_state) method to retrieve the graph state after calling `invoke` or `ainvoke`.

```md-code__content
# Run the graph up to the interrupt
result = graph.invoke(inputs, thread_config)
# Get the graph state to get interrupt information.
state = graph.get_state(thread_config)
# Print the state values
print(state.values)
# Print the pending tasks
print(state.tasks)
# Resume the graph with the user's input.
graph.invoke(Command(resume={"age": "25"}), thread_config)

```

```md-code__content
{'foo': 'bar'} # State values
(
    PregelTask(
        id='5d8ffc92-8011-0c9b-8b59-9d3545b7e553',
        name='node_foo',
        path=('__pregel_pull', 'node_foo'),
        error=None,
        interrupts=(Interrupt(value='value_in_interrupt', resumable=True, ns=['node_foo:5d8ffc92-8011-0c9b-8b59-9d3545b7e553'], when='during'),), state=None,
        result=None
    ),
) # Pending tasks. interrupts

```

## How does resuming from an interrupt work? [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#how-does-resuming-from-an-interrupt-work "Permanent link")

Warning

Resuming from an `interrupt` is **different** from Python's `input()` function, where execution resumes from the exact point where the `input()` function was called.

A critical aspect of using `interrupt` is understanding how resuming works. When you resume execution after an `interrupt`, graph execution starts from the **beginning** of the **graph node** where the last `interrupt` was triggered.

**All** code from the beginning of the node to the `interrupt` will be re-executed.

```md-code__content
counter = 0
def node(state: State):
    # All the code from the beginning of the node to the interrupt will be re-executed
    # when the graph resumes.
    global counter
    counter += 1
    print(f"> Entered the node: {counter} # of times")
    # Pause the graph and wait for user input.
    answer = interrupt()
    print("The value of counter is:", counter)
    ...

```

Upon **resuming** the graph, the counter will be incremented a second time, resulting in the following output:

```md-code__content
> Entered the node: 2 # of times
The value of counter is: 2

```

## Common Pitfalls [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#common-pitfalls "Permanent link")

### Side-effects [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#side-effects "Permanent link")

Place code with side effects, such as API calls, **after** the `interrupt` to avoid duplication, as these are re-triggered every time the node is resumed.

[Side effects before interrupt (BAD)](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#__tabbed_2_1)[Side effects after interrupt (OK)](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#__tabbed_2_2)[Side effects in a separate node (OK)](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#__tabbed_2_3)

This code will re-execute the API call another time when the node is resumed from
the `interrupt`.

This can be problematic if the API call is not idempotent or is just expensive.

```md-code__content
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""
    api_call(...) # This code will be re-executed when the node is resumed.
    answer = interrupt(question)

```

```md-code__content
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""

    answer = interrupt(question)

    api_call(answer) # OK as it's after the interrupt

```

```md-code__content
from langgraph.types import interrupt

def human_node(state: State):
    """Human node with validation."""

    answer = interrupt(question)

    return {
        "answer": answer
    }

def api_call_node(state: State):
    api_call(...) # OK as it's in a separate node

```

### Subgraphs called as functions [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#subgraphs-called-as-functions "Permanent link")

When invoking a subgraph [as a function](https://langchain-ai.github.io/langgraph/concepts/low_level/#as-a-function), the **parent graph** will resume execution from the **beginning of the node** where the subgraph was invoked (and where an `interrupt` was triggered). Similarly, the **subgraph**, will resume from the **beginning of the node** where the `interrupt()` function was called.

For example,

```md-code__content
def node_in_parent_graph(state: State):
    some_code()  # <-- This will re-execute when the subgraph is resumed.
    # Invoke a subgraph as a function.
    # The subgraph contains an `interrupt` call.
    subgraph_result = subgraph.invoke(some_input)
    ...

```

**Example: Parent and Subgraph Execution Flow**

Say we have a parent graph with 3 nodes:

**Parent Graph**: `node_1` â†’ `node_2` (subgraph call) â†’ `node_3`

And the subgraph has 3 nodes, where the second node contains an `interrupt`:

**Subgraph**: `sub_node_1` â†’ `sub_node_2` ( `interrupt`) â†’ `sub_node_3`

When resuming the graph, the execution will proceed as follows:

1. **Skip `node_1`** in the parent graph (already executed, graph state was saved in snapshot).
2. **Re-execute `node_2`** in the parent graph from the start.
3. **Skip `sub_node_1`** in the subgraph (already executed, graph state was saved in snapshot).
4. **Re-execute `sub_node_2`** in the subgraph from the beginning.
5. Continue with `sub_node_3` and subsequent nodes.

Here is abbreviated example code that you can use to understand how subgraphs work with interrupts.
It counts the number of times each node is entered and prints the count.

```md-code__content
import uuid
from typing import TypedDict

from langgraph.graph import StateGraph
from langgraph.constants import START
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
   """The graph state."""
   state_counter: int

counter_node_in_subgraph = 0

def node_in_subgraph(state: State):
   """A node in the sub-graph."""
   global counter_node_in_subgraph
   counter_node_in_subgraph += 1  # This code will **NOT** run again!
   print(f"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times")

counter_human_node = 0

def human_node(state: State):
   global counter_human_node
   counter_human_node += 1 # This code will run again!
   print(f"Entered human_node in sub-graph a total of {counter_human_node} times")
   answer = interrupt("what is your name?")
   print(f"Got an answer of {answer}")

checkpointer = MemorySaver()

subgraph_builder = StateGraph(State)
subgraph_builder.add_node("some_node", node_in_subgraph)
subgraph_builder.add_node("human_node", human_node)
subgraph_builder.add_edge(START, "some_node")
subgraph_builder.add_edge("some_node", "human_node")
subgraph = subgraph_builder.compile(checkpointer=checkpointer)

counter_parent_node = 0

def parent_node(state: State):
   """This parent node will invoke the subgraph."""
   global counter_parent_node

   counter_parent_node += 1 # This code will run again on resuming!
   print(f"Entered `parent_node` a total of {counter_parent_node} times")

   # Please note that we're intentionally incrementing the state counter
   # in the graph state as well to demonstrate that the subgraph update
   # of the same key will not conflict with the parent graph (until
   subgraph_state = subgraph.invoke(state)
   return subgraph_state

builder = StateGraph(State)
builder.add_node("parent_node", parent_node)
builder.add_edge(START, "parent_node")

# A checkpointer must be enabled for interrupts to work!
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {
   "configurable": {
      "thread_id": uuid.uuid4(),
   }
}

for chunk in graph.stream({"state_counter": 1}, config):
   print(chunk)

print('--- Resuming ---')

for chunk in graph.stream(Command(resume="35"), config):
   print(chunk)

```

This will print out

```md-code__content
Entered `parent_node` a total of 1 times
Entered `node_in_subgraph` a total of 1 times
Entered human_node in sub-graph a total of 1 times
{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['parent_node:4c3a0248-21f0-1287-eacf-3002bc304db4', 'human_node:2fe86d52-6f70-2a3f-6b2f-b1eededd6348'], when='during'),)}
--- Resuming ---
Entered `parent_node` a total of 2 times
Entered human_node in sub-graph a total of 2 times
Got an answer of 35
{'parent_node': {'state_counter': 1}}

```

### Using multiple interrupts [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#using-multiple-interrupts "Permanent link")

Using multiple interrupts within a **single** node can be helpful for patterns like [validating human input](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#validating-human-input). However, using multiple interrupts in the same node can lead to unexpected behavior if not handled carefully.

When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is critical.

To avoid issues, refrain from dynamically changing the node's structure between executions. This includes adding, removing, or reordering interrupt calls, as such changes can result in mismatched indices. These problems often arise from unconventional patterns, such as mutating state via `Command(resume=..., update=SOME_STATE_MUTATION)` or relying on global variables to modify the nodeâ€™s structure dynamically.

Example of incorrect code

```md-code__content
import uuid
from typing import TypedDict, Optional

from langgraph.graph import StateGraph
from langgraph.constants import START
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
    """The graph state."""

    age: Optional[str]
    name: Optional[str]

def human_node(state: State):
    if not state.get('name'):
        name = interrupt("what is your name?")
    else:
        name = "N/A"

    if not state.get('age'):
        age = interrupt("what is your age?")
    else:
        age = "N/A"

    print(f"Name: {name}. Age: {age}")

    return {
        "age": age,
        "name": name,
    }

builder = StateGraph(State)
builder.add_node("human_node", human_node)
builder.add_edge(START, "human_node")

# A checkpointer must be enabled for interrupts to work!
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {
    "configurable": {
        "thread_id": uuid.uuid4(),
    }
}

for chunk in graph.stream({"age": None, "name": None}, config):
    print(chunk)

for chunk in graph.stream(Command(resume="John", update={"name": "foo"}), config):
    print(chunk)

```

```md-code__content
{'__interrupt__': (Interrupt(value='what is your name?', resumable=True, ns=['human_node:3a007ef9-c30d-c357-1ec1-86a1a70d8fba'], when='during'),)}
Name: N/A. Age: John
{'human_node': {'age': 'John', 'name': 'N/A'}}

```

## Additional Resources ðŸ“š [Â¶](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\#additional-resources "Permanent link")

- [**Conceptual Guide: Persistence**](https://langchain-ai.github.io/langgraph/concepts/persistence/#replay): Read the persistence guide for more context on replaying.
- [**How to Guides: Human-in-the-loop**](https://langchain-ai.github.io/langgraph/how-tos/#human-in-the-loop): Learn how to implement human-in-the-loop workflows in LangGraph.
- [**How to implement multi-turn conversations**](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/): Learn how to implement multi-turn conversations in LangGraph.

## Comments

giscus

#### [2 reactions](https://github.com/langchain-ai/langgraph/discussions/2290)

ðŸ‘1â¤ï¸1

#### [12 comments](https://github.com/langchain-ai/langgraph/discussions/2290)

#### Â·

#### 9 replies

_â€“ powered by [giscus](https://giscus.app/)_

- Oldest
- Newest

[![@phfifofum](https://avatars.githubusercontent.com/u/178105107?v=4)phfifofum](https://github.com/phfifofum) [Nov 1, 2024](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11119690)

I have a backend, fast api and front end streamlit architecture.... struggling to get this working well.

I can edit and resume the graph fine from the back end and fast api using checkpointing.

What I cant do well (or cleanly) is trigger a graph update from streamlit front end so the events stream nicely into my existing graph invoke code and event/UI handling.

Any insights or code examples for this?

3

ðŸ‘6

2 replies

[![@idotr7](https://avatars.githubusercontent.com/u/116896107?v=4)](https://github.com/idotr7)

[idotr7](https://github.com/idotr7) [Nov 10, 2024](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11203952)

i have this problem a well, it's not clear how to make it work

ðŸ‘1

[![@DuncanRiv](https://avatars.githubusercontent.com/u/174958551?u=6b1e20eff420d3283a50025ff0e4f3f49fcba282&v=4)](https://github.com/DuncanRiv)

[DuncanRiv](https://github.com/DuncanRiv) [Jan 13](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11823290)

edited

If you are solely using streamlit on the frontend, this might not be possible. (I don't have much experience with streamlit). You can probably pull this off by making successive API calls with additional python libraries, like trying using streamlit in tandem with the "requests" python library.

Regardless,

I always recommend using a javascript library or just plain javascript for the frontend and just establish a streaming or event based API connection. It gives you a lot more flexibility and your UI/UX will get improved! WIN-WIN.

If you do choose to go down this route, a nice way that I have found to accomplish this is by routing between "Initializing"/"Starting"/"Continuing" the graph WITHOUT human input and "Resuming the graph" WITH human input. This is because your graph invocation is going to look different for both cases. All it takes is just a slightly modified post request from your frontend for each case.

_IMPORTANT_ On your frontend you need to keep track of your thread\_id.

I will give you an example just using astream because we want our applications to be as close to production-ready as possible:

Step 1: On your frontend, establish some boolean to decide whether to resume or not. Call it resume = False initially.

Note: I am only going to include the necessary details below because this is a lot to type out lol

Step 2: Setup your post API endpoint in FASTAPI roughly like this:

```notranslate
  async def apicall(state):

         try:
               if state[resume]: ****ASSUMING TYPEDICT (state.resume IF USING PYDANTIC)

                   THIS WILL HANDLE THE RESUMING WITH HUMAN INPUT

                async for chunk in graph.astream(resume= *insert variable needed to resume), config=thread_config, stream_mode=.
                 ["updates","custom"]):

                 print(chunk) etc.....


                 else:
                 THIS WILL HANDLE EVERYTHING ELSE
                 async for chunk in graph.astream({"messages":input_message},  config=thread_config, stream_mode=["updates","custom"]):
                 print(chunk) etc.....


  catch/Except:
       etc....

```

Step 3:

Setup the node you wish to interrupt in:

def One\_node(state:ChatbotState, writer:StreamWriter) -> Command:

print("I am in One Node")

```notranslate
question = "What node would you like to go to next?"

next_node = interrupt(question)

return Command(goto=next_node)

I know the details are rough, but hopefully you get the point: Just route between a human graph invocation and a regular graph invocation using a boolean from your frontend and remember to use the same thread_id!

```

If still not clear, don't worry I am going to make a tutorial soon on using LangGraph in combination with frontends. I know how confusing the continuously changing docs can get. I might even do my own version of a LangGraph course on YouTube soon just to supplement and clarify confusions. I will post a link here if that happens :)

[![@cris-m](https://avatars.githubusercontent.com/u/29815096?u=4b55bcd0d0e557e3cc2a483bfd427627d7e52493&v=4)cris-m](https://github.com/cris-m) [Dec 16, 2024](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11577506)

I have try this and I am still getting `HumanMessage` instead of `AIMessage` after resuming the state.

```
from langgraph.graph import StateGraph, MessagesState
from langgraph.types import interrupt, Command
from langgraph.checkpoint.memory import MemorySaver

def human_node(state: MessagesState):
    value = interrupt(f"What should I say in response to {state['messages']}")

    return {"messages": [{"role": "user", "content": value}]}

checkpointer = MemorySaver()
graph_builder = StateGraph(MessagesState)
graph_builder.add_node(human_node)
graph_builder.set_entry_point("human_node")
graph = graph_builder.compile(
    checkpointer=checkpointer
)
```

First call

```
thread_config = {"configurable": {"thread_id": "some_id"}}
graph.invoke({"messages": [{ "role": "user", "content": "Hello!"}]}, config=thread_config)
```

I go the following out put

`{'messages': [HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='dce77926-559a-4900-bf80-63e3879aecd9')]}`

Now resume the graph:

```
graph.invoke(Command(resume="Hello, How can i help you?"), config=thread_config)
```

The output is 2 `HummanMessage`:

```notranslate
{'messages': [HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='dce77926-559a-4900-bf80-63e3879aecd9'),\
  HumanMessage(content='Hello, How can i help you?', additional_kwargs={}, response_metadata={}, id='e20ad3e8-86d9-4bd1-a7b1-c17f88a8a421')]}

```

1

1 reply

[![@eyurtsev](https://avatars.githubusercontent.com/u/3205522?v=4)](https://github.com/eyurtsev)

[eyurtsev](https://github.com/eyurtsev) [Dec 16, 2024](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11582851)

Contributor

> I have try this and I am still getting HumanMessage instead of AIMessage after resuming the state.

The code that you shared is expected to be doing that. There's no node for a chat model to add an AIMessage.

What you're doing is:

1. `graph.invoke({"messages": [{ "role": "user", "content": "Hello!"}]}, config=thread_config)` writing a human message to the chat history
2. `value = interrupt(f"What should I say in response to {state['messages']}")` pausing immediately to collect human input
3. `graph.invoke(Command(resume="Hello, How can i help you?"), config=thread_config)` followed by: `return {"messages": [{"role": "user", "content": value}]}`


writing another human message to the chat history
4. ending the graph execution.

* * *

I recommend starting from reading the langgraph tutorial before doing any of the concept pages or how-to guides: [https://langchain-ai.github.io/langgraph/tutorials/introduction/](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

HIL isn't needed if the graph only contains a single agent. (In the shared code there isn't even a single agent / llm call.)

ðŸ‘2

[![@vigneshmj1997](https://avatars.githubusercontent.com/u/33595829?u=d0ee173f2ceb7074923d3ffbd63ca56493081977&v=4)vigneshmj1997](https://github.com/vigneshmj1997) [Jan 5](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11741901)

Hey i am using interrupt inside sub graph and when i saw the writes variable of the interrupt

it was only half filled Writes \[('00000000-0000-0000-0000-000000000000', ' **resume**', '3')\]

any idea why ?

1

0 replies

[![@vigneshmj1997](https://avatars.githubusercontent.com/u/33595829?u=d0ee173f2ceb7074923d3ffbd63ca56493081977&v=4)vigneshmj1997](https://github.com/vigneshmj1997) [Jan 5](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11741908)

Hey My interrupt is going in a infinite loop any solutions ?

1

1 reply

[![@eyurtsev](https://avatars.githubusercontent.com/u/3205522?v=4)](https://github.com/eyurtsev)

[eyurtsev](https://github.com/eyurtsev) [Jan 6](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11751612)

Contributor

Start here if you're looking for help: [https://stackoverflow.com/help/minimal-reproducible-example](https://stackoverflow.com/help/minimal-reproducible-example)

[![@mnuryar](https://avatars.githubusercontent.com/u/133870160?v=4)mnuryar](https://github.com/mnuryar) [Jan 29](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-11991538)

it is said that "Resuming from an interrupt is different from Python's input() function, where execution resumes from the exact point where the input() function was called.".

Is there a way to let execution resume from the exact point where interrupt() function was called, just like input() function?

1

0 replies

[![@mnuryar](https://avatars.githubusercontent.com/u/133870160?v=4)mnuryar](https://github.com/mnuryar) [Feb 2](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12030020)

edited

Let's say there is a graph with multiple human in the loop nodes

Below is the order of nodes in the graph:

step 1 node

human input node 1

step 2 node

human input node 2

I am streaming the graph with following:

```notranslate
for event in graph.stream(input, config):
	if '__interrupt__' in event.keys():
		human_input = "..."
		graph.stram(Comman(resume=human_input), config):
	else:
		for value in event.values()
			print(agent_response)

```

When 'human input node 1' hits the interrupt function, it stops the graph and resumes the graph execution with the 'human\_input" value.

After this, instead of going to 'step 2 node, it starts all over again from the very beginning of the graph (i.e., step 1 node). Basically, the 'interrupt' functions breaks the

for loop at the top, and remaining nodes ('step 2 node' and 'human input node 2') don't get executed.

The documentation says the interrupt function resumes from the node where it is applied, and I expected it to finish executing all the nodes in the graph.

Why does it exit the for loop?

1

4 replies

[![@avfranco-br](https://avatars.githubusercontent.com/u/20467839?u=5c3ec3af04b6f428ec7db760a0881e90b2267761&v=4)](https://github.com/avfranco-br)

[avfranco-br](https://github.com/avfranco-br) [Feb 2](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12031617)

edited

It seems not possible to interrupt and resume on the same streaming. I'm struggling to make this work. In my use case, the workflow (super\_graph, sub\_graph, Gradio) is working when the graph is tested from a Jupyter notebook making a first call to the graph, interrupt, next calling the Gradio app, then a new call to resume with the user input as code below. What I am trying to achieve, don't know if possible, is to trigger the call to \_get\_user\_input() and resume the graph directly from the graph structure (nodes). At this way, to use the graph it would be just a one call to the super\_graph but not working.

\`

async for chunk in super\_graph.astream(

{"messages": \[{"role": "user", "content": user\_input}\]},

config=thread\_config,

subgraphs=True,

):

print(chunk)

```notranslate
user_input = await _get_user_input()

async for chunk in super_graph.astream(
        Command(resume=user_input),
        config=thread_config,
        subgraphs=True,
    ):
        print(chunk)

```

\`

[![@mnuryar](https://avatars.githubusercontent.com/u/133870160?v=4)](https://github.com/mnuryar)

[mnuryar](https://github.com/mnuryar) [Feb 2](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12033432)

edited

essentially, I am trying to get the same thing as you do. The scenario I described is for my front end application using FastAPI and Websocket ( equivalent of Gradio in your case).

The examples in the documentation work great in Jupyter note book where the human in the loop has been implemented on graphs with very simple structure. As others were asking, the LangChain team hasn't shown an example where this can be implemented seamlessly in a production environment, both from backend and front end perspective.

ðŸ‘1

[![@avfranco-br](https://avatars.githubusercontent.com/u/20467839?u=5c3ec3af04b6f428ec7db760a0881e90b2267761&v=4)](https://github.com/avfranco-br)

[avfranco-br](https://github.com/avfranco-br) [Feb 4](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12058197)

> essentially, I am trying to get the same thing as you do. The scenario I described is for my front end application using FastAPI and Websocket ( equivalent of Gradio in your case).
>
> The examples in the documentation work great in Jupyter note book where the human in the loop has been implemented on graphs with very simple structure. As others were asking, the LangChain team hasn't shown an example where this can be implemented seamlessly in a production environment, both from backend and front end perspective.

Forgot to mention one thing that may help in your case. The first option I tried was to override the Interrupt class with a custom\_async\_interrupt function and class but as I'm not an expert handling async, multi-thread I got stuck and pivoted for now. However maybe something you can try it out.

ðŸ‘2

[![@mnuryar](https://avatars.githubusercontent.com/u/133870160?v=4)](https://github.com/mnuryar)

[mnuryar](https://github.com/mnuryar) [Feb 5](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12062768)

> Let's say there is a graph with multiple human in the loop nodes Below is the order of nodes in the graph:
>
> step 1 node human input node 1 step 2 node human input node 2
>
> I am streaming the graph with following:
>
> ```notranslate
> for event in graph.stream(input, config):
> 	if '__interrupt__' in event.keys():
> 		human_input = "..."
> 		graph.stram(Comman(resume=human_input), config):
> 	else:
> 		for value in event.values()
> 			print(agent_response)
>
> ```
>
> When 'human input node 1' hits the interrupt function, it stops the graph and resumes the graph execution with the 'human\_input" value. After this, instead of going to 'step 2 node, it starts all over again from the very beginning of the graph (i.e., step 1 node). Basically, the 'interrupt' functions breaks the for loop at the top, and remaining nodes ('step 2 node' and 'human input node 2') don't get executed.
>
> The documentation says the interrupt function resumes from the node where it is applied, and I expected it to finish executing all the nodes in the graph. Why does it exit the for loop?

I was able to figure out why it exists the for loop. The reason was due to anther while loop came before this.

But I got another problem.

The for loop was able to handle the 'human input node 1'. But when the execution reaches to the 'human input node 2', the code breaks because

`graph.stram(Comman(resume=human_input), config)`

wants to execute the rest of the graph till the end. I want the graph execute only 'human input 1' node with the line above, and go back to the for loop to execute the rest of the nodes.

Any ideas how I can do that?

[![@Gonzalob90](https://avatars.githubusercontent.com/u/13545820?u=e0670032e2d634bacb7aaa7e0cb3233f5305747f&v=4)Gonzalob90](https://github.com/Gonzalob90) [Feb 4](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12054501)

When multiple nodes raise an interrupt, it looks like only one interrupt can be pending at a time. Is there a way to handle multiple interrupts independently within the same graph?

1

0 replies

[![@azmathmoosa](https://avatars.githubusercontent.com/u/5759019?v=4)azmathmoosa](https://github.com/azmathmoosa) [20 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12274863)

I have a situation where I have a chat interface and a ticketing tool that needs to confirm with user. I want to know who caused the interrupt so I can prompt the user outside the graph with appropriate questions.

My ticketing tool has this code

```notranslate
print("Would you like to raise ticket with below description?")  # i want to move this outside graph
print(f"{issue_description}")   # it prints twice
print(f"Yes/No (default: No):")
waiting_for_user_flag.set()
user_confirmation = interrupt("ticket_confirmation")
if "yes" in user_confirmation.lower():
    user_confirmation = True
else:
    user_confirmation = False

```

and my user\_node code is

```notranslate
def node_user(state: State):
    # print("NODE:: node_user")
    waiting_for_user_flag.set()
    user = interrupt("get user input")
    return {"messages": user}

```

and this is how I am trying to loop

```notranslate
config = {"configurable":{"thread_id": 20}}
while not goodbye_flag.is_set():
    if waiting_for_user_flag.is_set():  #who caused the interrupt? was it tool or was it normal chat flow
        user = input("User: ")
        waiting_for_user_flag.clear()
        start_state = Command(resume=user)
    else:
        start_state = initial_state

    stream = app.stream(start_state, config=config, stream_mode="messages")
    for msg, metadata in stream:
        print(msg.content, end="")
        try:
            if msg.response_metadata['done']:
                print()
        except KeyError:
            pass

```

My problem is that I am not able to differentiate between who caused the interrupt? Since the tool node is called again when i resume, i am seeing it print two times the confirmation question. I was hoping that this would be a simple use case but its convoluted like crazy for no reason.

Can someone guide me to some code that handles similar case?

2

0 replies

[![@udaylunawat](https://avatars.githubusercontent.com/u/24354945?u=f1eb1d9248a0287547da38849ffbc0b01c931585&v=4)udaylunawat](https://github.com/udaylunawat) [17 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12304794)

edited

[@phfifofum](https://github.com/phfifofum) [@DuncanRiv](https://github.com/DuncanRiv) [@mnuryar](https://github.com/mnuryar) Can anyone of you provide with a minimal example of how you've implemented FastAPI with HITL using Interrupt? Or just point me to any resource. I'd be of great help!

1

1 reply

[![@willjrho](https://avatars.githubusercontent.com/u/177953434?u=b410b8a374f6c9bf2a5fde7493042d26a6239aea&v=4)](https://github.com/willjrho)

[willjrho](https://github.com/willjrho) [11 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12368037)

This is what i am looking for as well

[![@TillSimon](https://avatars.githubusercontent.com/u/738058?u=95fb22fa999d42f384abd6bf9a337dd0c6bd4606&v=4)TillSimon](https://github.com/TillSimon) [17 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12304868)

You can also have a look at the example **Sales Agent** here:

[https://github.com/gotohuman/examples-langgraph-py](https://github.com/gotohuman/examples-langgraph-py)

It's a FastAPI endpoint using the `interrupt` to wait for human approval/feedback and resumes with

```
await graph.ainvoke(Command(resume={ "response": approval, "reviewed_email": email_text, "comment": retry_comment }), config=thread_config)
```

Might also help you [@udaylunawat](https://github.com/udaylunawat) and [@mnuryar](https://github.com/mnuryar) [@azmathmoosa](https://github.com/azmathmoosa) (see how I pass values from and to the interrupt)

2

0 replies

[![@antoniowhbh](https://avatars.githubusercontent.com/u/169597778?v=4)antoniowhbh](https://github.com/antoniowhbh) [15 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12324601)

I have a problem, I'm using the interrupt and defined it in a separate node in my graph to use it with the agent inbox ui and hosted my graph locally using "langgraph dev". The interrupt actions are only functioning when the interrupt is on the first message of the thread, when I try to accept or edit or do any other action on the interrupt that is not on the first message on the thread it does nothing. if the I did this exactly the same way define in the agent inbox repo and I tried it with the examples that creates and interrupts them and confirms the jokes using the ui. I am also facing this exact problem with example graph in this repository [https://github.com/langchain-ai/agent-inbox-langgraph-example](https://github.com/langchain-ai/agent-inbox-langgraph-example) I don't know if the problem is in the graph itself or how the agent inbox ui processes and submits the interrupts. I tried looking at the source code of agent inbox but I couldn't find the issue

1

0 replies

[![@willjrho](https://avatars.githubusercontent.com/u/177953434?u=b410b8a374f6c9bf2a5fde7493042d26a6239aea&v=4)willjrho](https://github.com/willjrho) [11 days ago](https://github.com/langchain-ai/langgraph/discussions/2290#discussioncomment-12368030)

having a similar issue with interrupt exposed via an api. the graph crashes without an opportunity to get a state update from a second endpoint to resume. the intended behavior is:

-get to the interupt node in the graph

-return info to the user

-user confirms via second endpoint

-graph state updates and either continues to the next node or ends

1

0 replies

WritePreview

[Styling with Markdown is supported](https://guides.github.com/features/mastering-markdown/ "Styling with Markdown is supported")

[Sign in with GitHub](https://giscus.app/api/oauth/authorize?redirect_uri=https%3A%2F%2Flangchain-ai.github.io%2Flanggraph%2Fconcepts%2Fhuman_in_the_loop%2F)